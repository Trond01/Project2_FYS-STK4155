{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"8b971546199e46beacf9a722645832ab","deepnote_cell_type":"markdown"},"source":["Part a) Write your own Stochastic Gradient Descent Code\n","\n","### GD\n","Create\n","* [X] A plain gradient descent with a fixed learning rate which you tune\n","* [X] Add momentum\n","* [X] Compare convergence between the momentum model and one with fixed learning rate\n","\n","Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs:\n","\n","### SGD\n","Create\n","* [ ] A stochastic gradient descent with a fixed learning rate which you tune\n","* [ ] Add momentum\n","* [ ] Compare convergence between the momentum model and one with fixed learning rate\n","Discuss\n","* [ ] Discuss the results as functions of the various parameters (size of batches, number of epochs etc). \n","\n","### Adagrad\n","\n","* [ ] Implement Adagrad without momentum for GD\n","* [ ] Implement Adagrad with momentum for GD\n","* [ ] Implement Adagrad without momentum for SGD\n","* [ ] Implement Adagrad with momentum for SGD\n","\n","### RMSprop\n","\n","* [ ] Implement RMSprop\n","\n","### Adam\n","\n","* [ ] Implement Adaptive momement estimation \n","\n","### Replace gradient\n","* [ ] Choose Autograd or JAX\n","* [ ] Replace analytical gradient with the chosen\n"]},{"cell_type":"markdown","metadata":{"cell_id":"832e6a77845b4689a3f939b123624833","deepnote_cell_type":"markdown"},"source":["https://seaborn.pydata.org/generated/seaborn.heatmap.html\n"]},{"cell_type":"markdown","metadata":{"cell_id":"9a5165bf52d0489aae1f94a24d0de5ab","deepnote_cell_type":"markdown"},"source":["### Forslag Trond og Eirik\n","\n","Vi skal erstatte matrise inverteringen i OLS og Ridge med GD og SGD. For OLS skal vi løse:\n","\n","$$\n","    \\hat{\\beta}_{OLS} \n","    = \\textrm{arg}\\min_{\\beta\\in\\mathbb{R}^p} C_{OLS}(y-X\\beta), \n","$$\n","\n","$$\n","    C_{OLS}(y-X\\beta) = \\frac{1}{N} \\lVert y-X\\beta \\rVert_2^2. \n","$$\n","\n","For Ridge løser vi:\n","\n","$$\n","    \\hat{\\beta}_{Ridge} \n","    = \\textrm{arg}\\min_{\\beta\\in\\mathbb{R}^p} C_{Ridge}(y-X\\beta), \n","$$\n","\n","$$\n","    C_{Ridge}(y-X\\beta) = \\frac{1}{N} \\lVert y-X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_2. \n","$$\n","\n","For å benytte GD trenger vi analytiske uttrykk for gradienten til disse. Vi finner:\n","\n","$$\n","    \\nabla_{\\beta}C_{OLS} = \\frac{2}{N} X^T(X\\beta - y),\n","$$\n","\n","$$\n","    \\nabla_{\\beta}C_{Ridge} = 2 \\left( \\frac{1}{N} X^T(X\\beta - y) + \\lambda\\beta^T \\right).\n","$$\n"]},{"cell_type":"markdown","metadata":{"cell_id":"d165fee93c624f28a06c093492023d94","deepnote_cell_type":"markdown"},"source":["https://www.deeplearningbook.org/contents/optimization.html\n"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"5b9fd0e5804e42fdac4a540ea4770d09","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":8623,"execution_start":1697703764402,"source_hash":"19c75304"},"outputs":[],"source":["import numpy as np\n","import jax.numpy as jnp\n","from jax import grad, nn\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"eef271d94bc84e93ab2ead81df00c8f2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":19,"execution_start":1697629725049,"source_hash":null},"outputs":[],"source":["# simple function\n","def f(x):\n","    return 1 + x + x**2\n","\n","def feature_matrix(x, num_features):\n","    \"\"\"\n","    x: array with x values\n","    num_features: the degree of polynomial 1+x+...+x^p\n","\n","    returns:\n","    X: The feature matrix,a 2D numpy array with a column for each feature\n","    \"\"\"\n","\n","    return np.array([x**i for i in range(num_features)]).T[0]\n","\n","\n","def MSELoss(y, y_pred):\n","    \"\"\"MSE loss of prediction array.\n","\n","    Args:\n","        y (ndarray): Target values\n","        y_pred (ndarray): Predicted values\n","\n","    Returns:\n","        float: MSE loss\n","    \"\"\"\n","    return jnp.sum(jnp.power(y - y_pred, 2)) / y.shape[0]\n","\n","\n","def model(beta, X):\n","    return jnp.dot(X, beta)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"cf71bb13c58d4e8abfe581da10ae0433","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":17,"execution_start":1697629725052,"source_hash":null},"outputs":[],"source":["def OLS_grad(beta, X, y):\n","    n = y.shape[0]\n","    return 2*(np.dot(X.T, ( model(beta, X) - y))) / n\n","\n","def RIDGE_grad(beta, X, y, lam=0.01):\n","    n = y.shape[0]\n","    return 2*((np.dot(X.T, ( model(beta, X) - y)) / n) + lam*beta)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"d19c926a10d9428f8514f6cba74ad2c4","deepnote_cell_type":"markdown"},"source":["#### Gradient descent\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d320542febce45419172a48d52e2629b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":16,"execution_start":1697629725058,"source_hash":null},"outputs":[],"source":["def GD(grad_method, x, y, model, beta0, lr, n_epochs, gamma=None):\n","    \"\"\"\n","\n","    minimizes ...\n","\n","    PARAMS\n","    grad_method : the function estimeating the gradient of the function to minimise\n","    x : function input\n","    y : target function output\n","    beta0 : initial point in the beta space \n","    lr : learning rate, size of stept in direction of decrease\n","    n_epochs : number of iterations\n","    gamma : if given, perform with momentum parameter gamma\n","    \n","    RETURNS\n","\n","    \"\"\"\n","\n","    loss_list = []\n","\n","    # Make feature matrix\n","    X = feature_matrix(x, beta0.shape[0])\n","\n","    # Initialise\n","    betas = [beta0]\n","    v = np.zeros_like(beta0) # initial v for momentum\n","\n","    # Perform training\n","    for i in range(n_epochs):\n","        \n","        # Evaluate gradient at previous x\n","        grad = grad_method(betas[-1], X, y)\n","\n","        #grad = derivative(betas[-1])\n","        loss_list.append(MSELoss(y, model(betas[-1], X)))\n","        # Perform step\n","        if gamma is not None:\n","            v = gamma * v + lr * grad\n","            betas.append( betas[-1] - v )\n","        else:\n","            betas.append( betas[-1] - lr * grad)\n","    \n","    # Return the found values of x \n","    return betas, loss_list"]},{"cell_type":"code","execution_count":null,"metadata":{"allow_embed":false,"cell_id":"06cd968148ea493485ff8f8d84f999df","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":null},"outputs":[],"source":["def TODO_GD(grad_method, X, y, beta0 = None, test_cost_func = None, lr=0.01, n_epochs=50, gamma=None, store_all = False):\n","    \"\"\"\n","\n","\n","    \"\"\"\n","\n","    # If test_cost_func:\n","\n","    if store_all:\n","        betas = []\n","\n","    # Make feature matrix\n","    X = feature_matrix(x, beta0.shape[0])\n","\n","    # Initialise\n","    betas = [beta0]\n","    v = np.zeros_like(beta0) # initial v for momentum\n","\n","    # Perform training\n","    for i in range(n_epochs):\n","        \n","        # Evaluate gradient at previous x\n","        grad = grad_method(betas[-1], X, y)\n","\n","        #grad = derivative(betas[-1])\n","        loss_list.append(MSELoss(y, model(betas[-1], X)))\n","        # Perform step\n","        if gamma is not None:\n","            v = gamma * v + lr * grad\n","            betas.append( betas[-1] - v )\n","        else:\n","            betas.append( betas[-1] - lr * grad)\n","    \n","    # Return the found values of x \n","    return betas, loss_list"]},{"cell_type":"markdown","metadata":{"cell_id":"b8a2637b43d94722bff31a61a4b315e3","deepnote_cell_type":"markdown"},"source":["#### Tuning the learning rate\n","\n","TODO: we should at some point find an efficient way to tone many parameters at the same time etc..."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"700e602eaa354562922b04e25d7ca14c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":8241,"execution_start":1697633504968,"source_hash":null},"outputs":[],"source":["def lr_gamma_experiment(learning_rates, grad_method, x, y, beta0, n_epochs, gamma_values):\n","    \"\"\"\n","    Plot the final error for fixed number of epochs and different learning rates\n","    for multiple gamma values on the same plot.\n","    \"\"\"\n","\n","    plt.figure(figsize=(10, 6))\n","    colors = plt.cm.jet(np.linspace(0, 1, len(gamma_values)))\n","\n","    for idx, gamma in enumerate(gamma_values):\n","        final_errors = []\n","        for lr in learning_rates:\n","            betas, loss_list = GD(grad_method, x, y, beta0, lr, n_epochs, gamma)\n","            final_errors.append(loss_list[-1])\n","\n","        plt.plot(learning_rates, final_errors, color=colors[idx], label=f'Gamma = {gamma}')\n","    \n","    plt.xscale('log')\n","    plt.ylim(-1, 8)\n","    plt.legend()\n","    plt.title('Learning Rate and Gamma Experiment')\n","    plt.show()\n","\n","# Perform experiment\n","num_params = 5\n","num_points = 50\n","\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","beta0 = np.random.random((num_params, 1)) - 0.5\n","\n","learning_rates = np.logspace(start=-6, stop=0, num=100)\n","gamma_values = [0.1, 0.5, 0.9, 0.99]\n","lr_gamma_experiment(learning_rates, OLS_grad, x, y, beta0, 100, gamma_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2c1f452052db42f0a47bd663fbcf479a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":2495,"execution_start":1697631295000,"source_hash":null},"outputs":[],"source":["def lr_experiment(learning_rates, grad_method, x, y, beta0, n_epochs):\n","    \"\"\"\n","    Plot the final error for fixed number of epochs and different learning rates\n","    \"\"\"\n","\n","    final_errors = []\n","\n","    for lr in learning_rates:\n","        \n","        betas, loss_list = GD(grad_method, x, y, beta0, lr, n_epochs)\n","\n","        final_errors.append(loss_list[-1])\n","\n","    plt.xscale('log')\n","    # Set y-axis limits to [0, 2]\n","    plt.ylim(-1, 8)\n","    plt.plot(learning_rates, final_errors)\n","\n","# Perform experiment\n","num_params = 5\n","num_points = 50\n","\n","\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","beta0 = np.random.random((num_params, 1)) - 0.5\n","\n","learning_rates = np.logspace(start=-6, stop=0, num=100)\n","lr_experiment(learning_rates, OLS_grad, x, y, beta0, 100)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"d29e259243cf428f85c098becae4dd95","deepnote_cell_type":"markdown"},"source":["#### Experiment with and without momentum"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ac591169121f41f0b997e875348bb1a7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":51,"execution_start":1697629727476,"source_hash":null},"outputs":[],"source":["# Perform experiment\n","num_params = 5\n","num_points = 50\n","\n","\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","print(x[-2])\n","\n","beta0 = np.random.random((num_params, 1)) - 0.5\n","\n","betas, loss_list = GD(OLS_grad, x, f(x), beta0, 0.01, 100)\n","betas, loss_list_2 = GD(OLS_grad, x, f(x), beta0, 0.01, 100, gamma=0.5)\n","beta = betas[-1] \n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7b44720512ee4bed861aac38ec5958bc","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":204,"execution_start":1697629727520,"source_hash":null},"outputs":[],"source":["plt.plot(loss_list, label=\"No Gamma\")\n","plt.plot(loss_list_2, label=\"Gamma\")\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"cell_id":"7b70177991144fb68e0fdc2bb5bc90b0","deepnote_cell_type":"markdown"},"source":["#### Stochastic gradient descent"]},{"cell_type":"code","execution_count":null,"metadata":{"allow_embed":false,"cell_id":"268edc71a5eb41bb823fa07a960564d7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":136,"execution_start":1697629727730,"source_hash":null},"outputs":[],"source":["n = x.shape[0]\n","\n","M = 5 # Size of minibatch\n","m = int(n / M) # number of minis\n","\n","# Partition into minibatches\n","def random_partition(x, y, batch_size):\n","    batches = []\n","    n = y.shape[0]\n","    m = int(n / batch_size)\n","    for i in range(m):\n","        index = list(range(i*batch_size, (i+1)*batch_size))\n","        batches.append((x[index], y[index]))\n","\n","    return batches\n","\n","\n","def SGD(grad_method, x, y, beta0, lr, n_epochs, batch_size, gamma=None):\n","\n","\n","    loss_list = []\n","\n","    # Make feature matrix\n","    #X = feature_matrix(x, y, beta0.shape[0])\n","\n","    batches = random_partition(x, y, batch_size)\n","\n","    # Initialise\n","    betas = [beta0]\n","    v = np.zeros_like(beta0) # initial v for momentum\n","\n","    # Number of batches\n","    n = y.shape[0]\n","    m = int(n / batch_size) \n","\n","    X_true = feature_matrix(x, beta0.shape[0])\n","\n","    # Perform training\n","    for i in range(n_epochs):\n","\n","        for j in range(m):\n","            k = np.random.randint(m)\n","            x_b, y_b = batches[k]\n","\n","            X = feature_matrix(x_b, num_features=beta0.shape[0])\n","\n","            # Evaluate gradient at previous x\n","            grad = OLS_grad(betas[-1], X, y_b)\n","\n","            #grad = derivative(betas[-1])\n","            loss_list.append(MSELoss(y, model(betas[-1], X_true)))\n","            # Perform step\n","            if gamma is not None:\n","                v = gamma * v + lr * grad\n","                betas.append( betas[-1] - v )\n","            else:\n","                betas.append( betas[-1] - lr * grad)\n","    \n","    # Return the found values of x \n","    return betas, loss_list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ffa48f1ef46c42229954a89ef67a128a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":1245,"execution_start":1697629727774,"source_hash":null},"outputs":[],"source":["num_params = 5\n","num_points = 100\n","\n","beta0 = np.random.random((num_params, 1))\n","\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","\n","\n","betas, loss_list = SGD(OLS_grad, x, y, beta0, batch_size=3, lr=0.01, n_epochs=100)\n","\n","print(loss_list)\n","plt.plot(loss_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9a40aefe507e478a80c55d79f03717b5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":484,"execution_start":1697629729023,"source_hash":null},"outputs":[],"source":["print(loss_list)\n","plt.plot(loss_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"bed7017774f04a8a9bce730b23eb5cbe","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":402,"execution_start":1697629729518,"source_hash":null},"outputs":[],"source":["# Repeat these steps for stochastic gradient descent with mini batches and a given number of epochs. \n","#Use a tunable learning rate as discussed in the lectures from week 39. \n","#Discuss the results as functions of the various parameters (size of batches, number of epochs etc)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"b341980fee4a4eef89417eb2e8f1e6c4","deepnote_cell_type":"markdown"},"source":["#### Adagrad\n","\n","Adagrad is the method of scaling down the learning rate as we go...\n","\n","We initialise some learning rate $\\eta$\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"db43654fef7441ed993de6aac583856f","deepnote_cell_type":"image","deepnote_img_src":"image-20231014-152824.png"},"source":["<img src=\"image-20231014-152824.png\" width=\"\" align=\"\" />"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"50c13ccf45614ad3b8bddae1a56edd9b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":728,"execution_start":1697629729535,"source_hash":null},"outputs":[],"source":["def SGD_adagrad(grad_method, x, y, beta0, lr, n_epochs, batch_size, gamma=None, delta = 1e-8):\n","\n","    # Initial value for the tuneable learning rate\n","    eta = lr\n","\n","    # Including AdaGrad parameter to avoid possible division by zero\n","    delta  = 1e-8\n","\n","    # Initialise\n","    loss_list = []\n","    betas = [beta0]\n","    v = np.zeros_like(beta0) # initial v for momentum\n","\n","    # Partition\n","    batches = random_partition(x, y, batch_size)\n","\n","    # Number of batches\n","    n = y.shape[0]\n","    m = int(n / batch_size) \n","\n","    # For measuring loss\n","    X_true = feature_matrix(x, beta0.shape[0])\n","\n","    for epoch in range(n_epochs):\n","\n","        # reset accumulation variable\n","        r = 0\n","\n","        for i in range(m):\n","\n","            # Draw a batch and make feature matrix\n","            k = np.random.randint(m)\n","            x_b, y_b = batches[k]\n","\n","            X = feature_matrix(x_b, num_features=beta0.shape[0])\n","\n","            # Compute gradient of this sub-epoch\n","            gradients = grad_method(betas[-1], X, y_b)\n","    \n","            # Add to total gradient                \n","            r += gradients*gradients\n","\n","            # Adagrad scaling, learning rate is scaled down, append new result\n","            lr_times_grad = eta/(delta+np.sqrt(r)) * gradients\n","\n","            # Perform step\n","            if gamma is not None:\n","                v = gamma * v + lr_times_grad\n","                betas.append( betas[-1] - v )\n","            else:\n","                betas.append( betas[-1] - lr_times_grad)\n","\n","            # Add loss\n","            loss_list.append(MSELoss(y, model(betas[-1], X_true)))\n","\n","    \n","    # Return the found values of x \n","    return betas, loss_list\n","\n","num_params = 5\n","num_points = 100\n","\n","beta0 = np.random.random((num_params, 1))\n","\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","\n","betas, loss_list = SGD_adagrad(OLS_grad, x, y, beta0, batch_size=10, lr=0.01, n_epochs=100)\n","\n","print(loss_list)\n","plt.plot(loss_list)\n","plt.xlabel(\"Training step\")\n","plt.ylabel(\"MSE\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"c0848d126e8c47609e9efef2863c61ad","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":158,"execution_start":1697629730071,"source_hash":null},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"536428b26ed64ba38fbd3bcf0d90d913","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":23,"execution_start":1697629730084,"source_hash":null},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"62bde72c7bbb48ddba685163c7ae523c","deepnote_cell_type":"markdown"},"source":["#### RMS prop"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a345f79a520743a1944516962f3c7a0d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":21,"execution_start":1697629730085,"source_hash":null},"outputs":[],"source":["def SGD_RMS_prop(grad_method, x, y, beta0, lr, n_epochs, batch_size, gamma=0, delta = 1e-8):\n","\n","    eta = 0.01\n","\n","    # Including AdaGrad parameter to avoid possible division by zero\n","    delta  = 1e-8\n","\n","    # Accumulation variable\n","    rho = 0.05\n","\n","    # Initialise\n","    loss_list = []\n","    betas = [beta0]\n","    v = np.zeros_like(beta0) # initial v for momentum\n","\n","    # Partition\n","    batches = random_partition(x, y, batch_size)\n","\n","    # Number of batches\n","    n = y.shape[0]\n","    m = int(n / batch_size) \n","\n","    # For measuring loss\n","    X_true = feature_matrix(x, beta0.shape[0])\n","\n","    # Perform training\n","    for i in range(n_epochs):\n","        Giter = 0\n","\n","        # Iterate over the batches\n","        for j in range(m):\n","            k = np.random.randint(m)\n","            x_b, y_b = batches[k]\n","\n","            X = feature_matrix(x_b, num_features=beta0.shape[0])\n","\n","            # Evaluate gradient at previous x\n","            grad = (1.0/batch_size)*grad_method(betas[-1], X, y_b)\n","\n","            Giter = (rho*Giter+(1-rho)*grad*grad)\n","            #grad = derivative(betas[-1])\n","\n","            loss_list.append(MSELoss(y, model(betas[-1], X_true)))\n","            \n","            update = grad*eta/(delta+np.sqrt(Giter))\n","            # Perform step\n","            v = gamma * v + update\n","            betas.append( betas[-1] - v )\n","        \n","    # Return the found values of x \n","    return betas, loss_list"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b14936f18d9c42c2936fca6d0c5ae642","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":652,"execution_start":1697629730088,"source_hash":null},"outputs":[],"source":["num_params = 5\n","num_points = 100\n","\n","beta0 = np.random.random((num_params, 1))\n","\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","\n","betas, loss_list = SGD_RMS_prop(OLS_grad, x, y, beta0, batch_size=10, lr=0.01, n_epochs=100)\n","\n","print(loss_list)\n","plt.plot(loss_list)\n","plt.xlabel(\"Training step\")\n","plt.ylabel(\"MSE\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"4bc4a728af52414a96765db44b681936","deepnote_cell_type":"markdown"},"source":["#### Adam\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"d57ea25cd7394e9f9bf612b8909570c5","deepnote_cell_type":"image","deepnote_img_src":"image-20231014-160214.png"},"source":["<img src=\"image-20231014-160214.png\" width=\"\" align=\"\" />"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fca2db4c56ad4742b262fafcd9f18c22","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":702,"execution_start":1697636438571,"source_hash":null},"outputs":[],"source":["\n","def SGD_adam(grad_method, x, y, beta0:dict, n_epochs, batch_size, model_test = None, lr=0.01, gamma=0.0, delta = 1e-8, beta1=0.9, beta2=0.99):\n","\n","    # Initial value for learning rate\n","    eta = lr\n","\n","\n","    keys = beta0.keys()\n","\n","    # Including AdaGrad parameter to avoid possible division by zero\n","    delta  = 1e-8\n","\n","    # Initialise\n","    loss_list = []\n","    betas = [beta0]\n","    v = np.zeros_like(beta0) # initial v for momentum\n","\n","    # Partition\n","    batches = random_partition(x, y, batch_size)\n","    # Number of batches\n","    n = y.shape[0]\n","    m = int(n / batch_size) \n","\n","    # For measuring loss\n","    X_true = feature_matrix(x, beta0[\"W1\"].shape[0])\n","\n","    s = {}\n","    r = {}\n","    v = {}\n","\n","    for key in keys:\n","        v[key] = np.zeros_like(beta0[key])\n","\n","    for epoch in range(n_epochs):\n","\n","        # Accumulation variable\n","        for key in keys:\n","            s[key] = 0\n","            r[key] = 0\n","        #s = 0 # first moment\n","        #r = 0 # second moment\n","\n","        for i in range(m):\n","            \n","            new_beta = {}\n","\n","            # Draw a batch and make feature matrix\n","            k = np.random.randint(m)\n","            x_b, y_b = batches[k]\n","            X = feature_matrix(x_b, num_features=beta0[\"W1\"].shape[0])\n","\n","            # Compute gradient of this sub-epoch\n","            gradients = grad_method(betas[-1], X, y_b)\n","\n","\n","            for key in keys:\n","                # Accumulate\n","                s[key] = beta1*s[key] + (1-beta1)*gradients[key]\n","                r[key] = beta2*r[key] + (1-beta2)*gradients[key]*gradients[key]\n","\n","                first_term = s[key]/(1-beta1**(epoch+1))            \n","                second_term = r[key]/(1-beta2**(epoch+1))\n","\n","                # Adam scaling\n","                update = eta*first_term / (np.sqrt(second_term) + delta) # safe division with delta\n","\n","                v[key] = gamma * v[key] + update\n","                # Perform step\n","                new_beta[key] = betas[-1][key] - v[key]\n","\n","                # Add loss\n","            \n","            loss_list.append(MSELoss(y, model_test(betas[-1], X_true)))\n","            betas.append(new_beta)\n","    \n","    # Return the found values of x \n","    return betas, loss_list\n","\n","num_params = 5\n","num_points = 100\n","\n","beta0 = np.random.random((num_params, 1))\n","\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","\n","#betas, loss_list = SGD_adam(OLS_grad, x, y, beta0, batch_size=10, n_epochs=100)\n","\n","# print(loss_list)\n","# plt.plot(loss_list)\n","# plt.xlabel(\"Training step\")\n","# plt.ylabel(\"MSE\")\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"f1a29ff587174eb48332f968d3a99161","deepnote_cell_type":"markdown"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"dccddcef9cff47a8a5a14e196aaffa39","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":82,"execution_start":1697636442038,"source_hash":null},"outputs":[],"source":["def Ridge_loss_method(lam, model):\n","    return (lambda beta, X, y : Ridge_Loss(beta, X, y, model, lam))\n","\n","def Ridge_Loss(beta, X, y, model, lam=0.01):\n","    return MSELoss(model(beta, X), y) + jnp.sum(jnp.power(beta, 2))*(lam/(2*jnp.size(y)))\n","\n","\n","def MSELoss_method(model):\n","    return (lambda beta, X, y : MSELoss(model(beta, X), y))\n","\n","\n","def MSELoss(y, y_pred):\n","    \"\"\"MSE loss of prediction array.\n","\n","    Args:\n","        y (ndarray): Target values\n","        y_pred (ndarray): Predicted values\n","\n","    Returns:\n","        float: MSE loss\n","    \"\"\"\n","    return jnp.sum(jnp.power(y - y_pred, 2)) / y.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"80d71bb6ae1748c081f13ecb478f23e7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":607,"execution_start":1697703662686,"source_hash":null},"outputs":[{"ename":"NameError","evalue":"name 'MSELoss_method' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     X_2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39msigmoid(X_2)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_2\n\u001b[0;32m---> 13\u001b[0m OLS_loss_2 \u001b[38;5;241m=\u001b[39m \u001b[43mMSELoss_method\u001b[49m(model\u001b[38;5;241m=\u001b[39mmodel_2)\n\u001b[1;32m     15\u001b[0m MSE_grad \u001b[38;5;241m=\u001b[39m jax_loss_grad(OLS_loss_2)\n\u001b[1;32m     17\u001b[0m Ridge_grad_jax \u001b[38;5;241m=\u001b[39m jax_loss_grad(loss_func\u001b[38;5;241m=\u001b[39mRidge_loss_method(\u001b[38;5;241m0.1\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel))\n","\u001b[0;31mNameError\u001b[0m: name 'MSELoss_method' is not defined"]}],"source":["def jax_loss_grad(loss_func):\n","    return grad(loss_func)\n","\n","\n","def model_2(beta, X):\n","    X_1 = jnp.dot(X, beta[\"W1\"])\n","    X_1 = nn.sigmoid(X_1)\n","    X_2 = jnp.dot(X_1, beta[\"W2\"]) + beta[\"b2\"]\n","    X_2 = nn.sigmoid(X_2)\n","    return X_2\n","\n","\n","OLS_loss_2 = MSELoss_method(model=model_2)\n","\n","MSE_grad = jax_loss_grad(OLS_loss_2)\n","\n","Ridge_grad_jax = jax_loss_grad(loss_func=Ridge_loss_method(0.1, model=model))\n","OLS_grad_jax = jax_loss_grad(loss_func=MSELoss_method(model))\n","\n","\n","num_params = 5\n","middle_layer  = 10\n","num_points = 100\n","\n","beta_try = {\"W1\":np.random.random((num_params, middle_layer)), \"W2\":np.random.random((middle_layer, 1)), \"b2\":np.random.random((1, 1))}\n","\n","\n","\n","beta0 = np.random.random((num_params, 1))\n","x = np.random.random((num_points, 1))\n","y = f(x)\n","\n","#print(test_grad(beta0, feature_matrix(X, num_params), y))\n","\n","# betas, loss_list = GD(OLS_grad_jax, x, y, beta0, lr=0.01, n_epochs=100, gamma=0.88)\n","\n","# print(loss_list)\n","# plt.plot(loss_list)\n","# plt.title(\"GD\")\n","# plt.xlabel(\"Training step\")\n","# plt.ylabel(\"MSE\")\n","# plt.show()\n","\n","\n","\n","betas, loss_list = SGD(OLS_grad_jax, x, y, beta0, batch_size=10, lr=0.01, n_epochs=100, gamma=0.99)\n","\n","print(loss_list)\n","plt.plot(loss_list)\n","plt.title(\"SGD\")\n","plt.xlabel(\"Training step\")\n","plt.ylabel(\"MSE\")\n","plt.show()\n","\n","# betas, loss_list = SGD_adagrad(OLS_grad_jax, x, y, beta0, batch_size=10, lr=0.01, gamma=0.99,delta=0.001, n_epochs=100)\n","\n","# print(loss_list)\n","# plt.plot(loss_list)\n","# plt.title(\"Adagrad\")\n","# plt.xlabel(\"Training step\")\n","# plt.ylabel(\"MSE\")\n","# plt.show()\n","\n","\n","# betas, loss_list = SGD_RMS_prop(OLS_grad_jax, x, y, beta0, batch_size=10, lr=0.01, gamma=0.99,delta=0.001, n_epochs=100)\n","\n","# print(loss_list)\n","# plt.plot(loss_list)\n","# plt.title(\"RMS_prop\")\n","# plt.xlabel(\"Training step\")\n","# plt.ylabel(\"MSE\")\n","# plt.show()\n","\n","\n","betas, loss_list = SGD_adam(MSE_grad, x, y, beta_try, lr=0.01, model_test=model_2, batch_size=10, gamma=0.0,delta=0.001, n_epochs=100)\n","\n","plt.plot(loss_list)\n","plt.title(\"Adam\")\n","plt.xlabel(\"Training step\")\n","plt.ylabel(\"MSE\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"37683841016c411a98a30f389631d7c3","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"951637df47734495ab09bf04e8669fde","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3d717f43-f565-4198-bc5d-42467675cd81' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"98e109c4563b4e4686a12aa4f4ae3743","deepnote_persisted_session":{"createdAt":"2023-10-18T14:15:15.938Z"},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
