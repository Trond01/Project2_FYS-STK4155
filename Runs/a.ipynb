{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for figures in problem a)\n",
    "\n",
    "In this notebook we do experiments with minimising OLS and Ridge cost functions for the Franke function for different methods and hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from Code.descent_methods import *\n",
    "\n",
    "# Import the gradient methods\n",
    "from Code.utilities import train_test_split, MSELoss_method\n",
    "\n",
    "# Import the gradient methods\n",
    "from Code.utilities import OLS_train_analgrad, OLS_train_autograd, ridge_train_analgrad, ridge_train_autograd\n",
    "\n",
    "methods_dict_list = [\n",
    "    {\"name\"   : \"GD\",       \"method\" : GD},\n",
    "    {\"name\"   : \"SGD\",      \"method\" : SGD},\n",
    "    {\"name\"   : \"adagrad\",  \"method\" : SGD_adagrad},\n",
    "    {\"name\"   : \"RMS prop\", \"method\" : SGD_RMS_prop},\n",
    "    {\"name\"   : \"adam\",     \"method\" : SGD_adam}\n",
    "]\n",
    "\n",
    "# Import tools from the first project \n",
    "from Code.project1_tools import feature_matrix_2d, r2_sampling, scale_feature_matrix\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise beta\n",
    "num_features = 10\n",
    "\n",
    "# Start with small values\n",
    "beta0 = {\"b\" : np.random.random(num_features)*0.1}\n",
    "\n",
    "# Sample points and split in train and test\n",
    "num_points = 100\n",
    "split=0.2\n",
    "data = r2_sampling(num_points)\n",
    "x, y, z = data[\"x\"], data[\"y\"], data[\"z\"]\n",
    "\n",
    "# Scaling etc...\n",
    "X = feature_matrix_2d(x, y, num_features)\n",
    "X, means, var = scale_feature_matrix(X)\n",
    "X_train, y_train, X_test, y_test, test_index = train_test_split(X, z, split)\n",
    "\n",
    "# Squeeze to avoid errors due to shape [100, 1]\n",
    "y_train, y_test = np.squeeze(y_train), np.squeeze(y_test)\n",
    "\n",
    "# Define the polynomial fit model\n",
    "def model(beta, X):\n",
    "    return jnp.dot(X, beta[\"b\"])\n",
    "\n",
    "loss_func = MSELoss_method(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the 4 gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use even number (:\n",
    "n_epochs = 1000\n",
    "\n",
    "# Initialise test parameters\n",
    "lam=0.1\n",
    "lr = 0.05\n",
    "\n",
    "# Loop\n",
    "linestyles = [\"r\", \"b--\", \"black\", \"g--\"]\n",
    "labels = [\"OLS anal\", \"OLS auto\", \"Ridge anal\", \"Ridge auto\"]\n",
    "for i, loss_grad in enumerate([OLS_train_analgrad(model), OLS_train_autograd(model), \n",
    "                  ridge_train_analgrad(model, lam), ridge_train_autograd(model, lam)]):\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    result_GD = GD(X_train, y_train, X_test, y_test, \n",
    "                grad_method=loss_grad, \n",
    "                n_epochs=n_epochs, \n",
    "                lr=lr,\n",
    "                beta0=beta0, \n",
    "                test_loss_func=(MSELoss_method(model)))\n",
    "\n",
    "    print(time.time()-t0)\n",
    "\n",
    "    # Plot the second half of training\n",
    "    plt.plot(np.arange(n_epochs//2, n_epochs+1, 1), result_GD[\"train_loss_list\"][n_epochs//2::], linestyles[i], label=labels[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that anal and auto give same results, but automatic takes 10* as much time ---> we use analytic in the following!\n",
    "\n",
    "Note also that Ridge takes twice as long for auto. This might be because we have 2 terms of jnpsum(jnppower)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# Fixed parametres\n",
    "epochs = 100\n",
    "\n",
    "# Values for experiment\n",
    "lr_vals = np.logspace(-2, 1, 6)\n",
    "gamma_vals = np.array([0.01, 0.05, 0.2, 0.5, 1])\n",
    "results = np.zeros((len(lr_vals), len(gamma_vals)))\n",
    "\n",
    "# We juse analytic ridge\n",
    "grad_method= OLS_train_analgrad(model)\n",
    "\n",
    "# Perform algorithm for each value\n",
    "for i, lr in enumerate(lr_vals):\n",
    "    for j, gamma in enumerate(gamma_vals):\n",
    "\n",
    "        # Perform experiment\n",
    "        result_GD = GD(X_train, y_train, X_test, y_test, \n",
    "                    grad_method=grad_method, \n",
    "                    n_epochs=epochs, \n",
    "                    lr=lr,\n",
    "                    beta0=beta0, \n",
    "                    gamma=gamma,\n",
    "                    test_loss_func=loss_func)\n",
    "\n",
    "        # Append the final error from training\n",
    "        results[i, j] = result_GD[\"test_loss_list\"][-1]\n",
    "\n",
    "\n",
    "# Make plot\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "df = pd.DataFrame(results, index=lr_vals, columns=gamma_vals)\n",
    "sns.heatmap(df, annot=True, ax=ax, cmap=\"viridis\",                  \n",
    "            xticklabels=df.columns.values.round(4),\n",
    "            yticklabels=df.index.values.round(4))\n",
    "ax.set_title(\"Final test MSE loss\")\n",
    "ax.set_ylabel(\"learning rate $\\eta$\")\n",
    "ax.set_xlabel(\"momentum parameter $\\gamma$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the methods for different number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We compare final train error to see \"how far convergence has come\"\n",
    "\n",
    "# Use OLS analytic\n",
    "loss_grad = OLS_train_analgrad(model)\n",
    "lr = 0.01\n",
    "\n",
    "# Max epoch number\n",
    "n_epochs = 1000\n",
    "\n",
    "def epoch_experiment(lr, n_epochs=n_epochs, loss_grad = loss_grad, methods_dict_list=methods_dict_list):\n",
    "\n",
    "    # For methods with n_batches, the default 5 is used!\n",
    "    n_batches = 5\n",
    "\n",
    "    for method_dict in methods_dict_list:\n",
    "\n",
    "        method_func = method_dict[\"method\"]\n",
    "        method_name = method_dict[\"name\"]\n",
    "\n",
    "        result = method_func(X_train, y_train, X_test, y_test, \n",
    "                grad_method=loss_grad, \n",
    "                n_epochs=n_epochs, \n",
    "                lr=lr,\n",
    "                beta0=beta0, \n",
    "                test_loss_func=(MSELoss_method(model)))\n",
    "            \n",
    "        if method_name==\"GD\": # batch size 1\n",
    "            plt.plot(result[\"train_loss_list\"][::1], label=method_name)\n",
    "        else:\n",
    "            plt.plot(result[\"train_loss_list\"][::n_batches], label=method_name)\n",
    "        print(f\"Final error for {method_name} = {result['train_loss_list'][-1]}\")\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# epoch_experiment(0.001)\n",
    "epoch_experiment(0.01)\n",
    "# epoch_experiment(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We compare final train error to see \"how far convergence has come\"\n",
    "\n",
    "# Use OLS analytic\n",
    "loss_grad = OLS_train_analgrad(model)\n",
    "lr = 0.01\n",
    "\n",
    "\n",
    "lr_values = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "def lr_experiment(lr_values, n_epochs=200, methods_dict_list=methods_dict_list):\n",
    "\n",
    "    # For methods with n_batches, the default 5 is used!\n",
    "    n_batches = 5\n",
    "\n",
    "    for method_dict in methods_dict_list:\n",
    "\n",
    "        method_func = method_dict[\"method\"]\n",
    "        method_name = method_dict[\"name\"]\n",
    "\n",
    "        final_errors = []\n",
    "\n",
    "        for n_epochs in n_epochs_to_test:\n",
    "\n",
    "\n",
    "            result = method_func(X_train, y_train, X_test, y_test, \n",
    "                    grad_method=loss_grad, \n",
    "                    n_epochs=n_epochs, \n",
    "                    lr=lr,\n",
    "                    beta0=beta0, \n",
    "                    test_loss_func=(MSELoss_method(model)))\n",
    "            \n",
    "            final_errors.append(result[\"train_loss_list\"][-1])\n",
    "\n",
    "        plt.plot(n_epochs_to_test, final_errors, label=method_name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "lr_experiment(lr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### analysis of results for OLS and RIDGE as function of\n",
    "- lr\n",
    "- #mini batches\n",
    "- #epochs\n",
    "- algorithm\n",
    "\n",
    "- lambda for ridge!\n",
    "    - use seabord ... to show results as function of lr and lam !\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_init(layer_list):\n",
    "    \"\"\"\n",
    "    layer list, eg [2, 10, 1] for 2 input, 10 hidden neurons and 1 output\n",
    "    \"\"\"\n",
    "\n",
    "    beta0 = {}\n",
    "\n",
    "    # Add random initialisation\n",
    "    for i in range(1, len(layer_list)):\n",
    "        # Weight matrix\n",
    "        beta0[f\"W{i}\"] = np.random.random((layer_list[i - 1], layer_list[i]))\n",
    "\n",
    "        # Bias vector\n",
    "        beta0[f\"b{i}\"] = np.random.random(layer_list[i])\n",
    "\n",
    "    return beta0\n",
    "\n",
    "def make_neural_network_string(beta):\n",
    "    \"\"\"\n",
    "    beta determines the architecture of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # Begin definition\n",
    "    function_defining_string = \"\"\"\n",
    "    \n",
    "    def neural_network_model(beta, X, activation=sigmoid, output_activation = (lambda x: x)):\n",
    "\n",
    "        out = X.copy()\n",
    "\n",
    "        # For each remaining layer we propagate forward\n",
    "        for i in range(1, len(beta.keys()) // 2):  # for each layer\n",
    "\n",
    "            # Dot with weights, add biases, apply activation function\n",
    "            out = activation(jnp.add(jnp.dot(out, beta[f\"W{i}\"]), beta[f\"b{i}\"]))\n",
    "\n",
    "    \"\"\"     \n",
    "\n",
    "    # Add hidden layer computation   \n",
    "\n",
    "    function_defining_string +=    \"\"\"\n",
    "        out_final = output_activation(jnp.add(\n",
    "            jnp.dot(out, beta[f\"W{len(beta.keys())//2}\"]), beta[f\"b{len(beta.keys())//2}\"]\n",
    "        ))\n",
    "\n",
    "        return out_final\n",
    "    \"\"\"\n",
    "\n",
    "    return string\n",
    "\n",
    "def neural_network_model(beta, X, activation=sigmoid, output_activation = (lambda x: x)):\n",
    "    \"\"\"\n",
    "    Function to evaluate the neural network prediction for feature matrix X\n",
    "    \"\"\"\n",
    "    # First layer = input\n",
    "    out = X.copy()\n",
    "\n",
    "    # For each remaining layer we propagate forward\n",
    "    for i in range(1, len(beta.keys()) // 2):  # for each layer\n",
    "\n",
    "        # Dot with weights, add biases, apply activation function\n",
    "        out = activation(jnp.add(jnp.dot(out, beta[f\"W{i}\"]), beta[f\"b{i}\"]))\n",
    "\n",
    "    out_final = output_activation(jnp.add(\n",
    "        jnp.dot(out, beta[f\"W{len(beta.keys())//2}\"]), beta[f\"b{len(beta.keys())//2}\"]\n",
    "    ))\n",
    "\n",
    "    return out_final\n",
    "\n",
    "beta = beta_init([2, 3, 3, 1])\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(\"print('hello')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn Ridge\n",
    "Ridge as function of learning rate and lambda\n",
    "\n",
    "use seabord ... to show results as function of lr and lam !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# Fixed parametres\n",
    "epochs = 10\n",
    "\n",
    "# Values for experiment\n",
    "lr_vals = np.logspace(-3, 1, 6)\n",
    "lam_vals = np.logspace(-10, 10, 6)\n",
    "results = np.zeros((len(lr_vals), len(lam_vals)))\n",
    "\n",
    "# We juse analytic ridge\n",
    "train_grad = ridge_train_analgrad\n",
    "\n",
    "# Perform algorithm for each value\n",
    "for i, lr in enumerate(lr_vals):\n",
    "    for j, lam in enumerate(lam_vals):\n",
    "\n",
    "        # Get gradient function for given lambda\n",
    "        grad_method = train_grad(model, lam)\n",
    "\n",
    "        # Perform experiment\n",
    "        result_GD = SGD_adam(X_train, y_train, X_test, y_test, \n",
    "                    grad_method=grad_method, \n",
    "                    n_epochs=epochs, \n",
    "                    n_batches=5,\n",
    "                    lr=lr,\n",
    "                    beta0=beta0, \n",
    "                    test_loss_func=loss_func)\n",
    "\n",
    "        # Append the final error from training\n",
    "        results[i, j] = result_GD[\"test_loss_list\"][-1]\n",
    "\n",
    "\n",
    "# Make plot\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "df = pd.DataFrame(results, index=lr_vals, columns=lam_vals)\n",
    "sns.heatmap(df, annot=True, ax=ax, cmap=\"viridis\",                  \n",
    "            xticklabels=df.columns.values.round(4),\n",
    "            yticklabels=df.index.values.round(4))\n",
    "ax.set_title(\"Final test MSE loss\")\n",
    "ax.set_ylabel(\"learning rate $\\eta$\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
