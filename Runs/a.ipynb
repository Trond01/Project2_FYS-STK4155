{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for figures in problem a)\n",
    "\n",
    "In this notebook we do experiments with minimising OLS and Ridge cost functions for the Franke function for different methods and hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling paths for importing code\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import split and MSELoss. All descent methods.\n",
    "from Code.utilities import train_test_split, MSELoss_method\n",
    "from Code.descent_methods import *\n",
    "\n",
    "# Import tools from the first project. Feature matrix, sampling and scaling\n",
    "from Code.project1_tools import feature_matrix_2d, r2_sampling, scale_feature_matrix\n",
    "\n",
    "# lines used for nice legend, ticker for x and y axis ticks, seaborn for grid search, pandas for nice datastorage. Numpy.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Timing methods\n",
    "import time as time\n",
    "\n",
    "# Import the gradient methods. Assign colors for plots comparing them ...\n",
    "from Code.utilities import OLS_train_analgrad, OLS_train_autograd, ridge_train_analgrad, ridge_train_autograd\n",
    "\n",
    "methods_dict_list = [{\"name\"   : \"GD\",       \"method\" : GD,           \"color\" : \"C0\"},\n",
    "                     {\"name\"   : \"SGD\",      \"method\" : SGD,          \"color\" : \"C1\"},\n",
    "                     {\"name\"   : \"adagrad\",  \"method\" : SGD_adagrad,  \"color\" : \"C2\"},\n",
    "                     {\"name\"   : \"RMS prop\", \"method\" : SGD_RMS_prop, \"color\" : \"C3\"},\n",
    "                     {\"name\"   : \"adam\",     \"method\" : SGD_adam,     \"color\" : \"C4\"}]\n",
    "\n",
    "# Set the default font size for text elements (e.g., titles, labels)\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 16  \n",
    "mpl.rcParams['legend.fontsize'] = 14\n",
    "sns.set() # default from sns is pretties (:\n",
    "\n",
    "# Set filename start. Create the folder if gone\n",
    "filepath_location = \"Figures/a/\"\n",
    "if not os.path.exists(filepath_location):\n",
    "    os.makedirs(filepath_location[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix a seed for sampling and initialisation to get consistent results\n",
    "np.random.seed(41)\n",
    "\n",
    "# Initialise beta\n",
    "num_features = 10\n",
    "\n",
    "# Start with small values\n",
    "beta0 = {\"b\" : np.random.random(num_features)*0.1}\n",
    "\n",
    "# Sample points and split in train and test\n",
    "num_points = 100\n",
    "split=0.2\n",
    "data = r2_sampling(num_points)\n",
    "x, y, z = data[\"x\"], data[\"y\"], data[\"z\"]\n",
    "\n",
    "# Scaling etc\n",
    "X = feature_matrix_2d(x, y, num_features)\n",
    "X, means, var = scale_feature_matrix(X)\n",
    "X_train, y_train, X_test, y_test, test_index = train_test_split(X, z, split)\n",
    "\n",
    "# Squeeze to avoid errors due to shape [100, 1]\n",
    "y_train, y_test = np.squeeze(y_train), np.squeeze(y_test)\n",
    "\n",
    "# Define the polynomial fit model. Define corresponding loss function\n",
    "def model(beta, X):\n",
    "    return jnp.dot(X, beta[\"b\"])\n",
    "\n",
    "loss_func = MSELoss_method(model)\n",
    "\n",
    "# Update filename string with parameters that are not experimented with below\n",
    "filepath_location_default = filepath_location + f\"deg={num_features}_N={num_points}_split={split}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the 4 gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_comparison_experiment(method=GD, savefig=False):\n",
    "    \n",
    "    # Get method name\n",
    "    method_name = next(item[\"name\"] for item in methods_dict_list if item[\"method\"] == method)\n",
    "    \n",
    "    # Use even number for epochs. Initialise test parameters\n",
    "    n_epochs = 1000\n",
    "    lam=0.1\n",
    "    lr = 0.05\n",
    "\n",
    "    # Make plot\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # Loop\n",
    "    linestyles = [\"r\", \"b--\", \"black\", \"g--\"]\n",
    "    labels = [\"OLS analytic\", \"OLS automatic\", \"Ridge analytic\", \"Ridge automatic\"]\n",
    "    for i, loss_grad in enumerate([OLS_train_analgrad(model), OLS_train_autograd(model), \n",
    "                    ridge_train_analgrad(model, lam), ridge_train_autograd(model, lam)]):\n",
    "\n",
    "        # Time the run\n",
    "        t0 = time.time()\n",
    "        result_GD = method(X_train, y_train, X_test, y_test, \n",
    "                    grad_method=loss_grad, \n",
    "                    n_epochs=n_epochs, \n",
    "                    lr=lr,\n",
    "                    beta0=beta0, \n",
    "                    test_loss_func=(MSELoss_method(model)))\n",
    "        print(f\"{labels[i]}: {round(time.time()-t0, 2)} s\")\n",
    "\n",
    "        # Plot the second half of training\n",
    "        plt.plot(np.arange(n_epochs//2, n_epochs+1, 1), result_GD[\"train_loss_list\"][n_epochs//2::], linestyles[i], label=labels[i])\n",
    "\n",
    "    plt.title(\"Comparing gradients\")\n",
    "    plt.xlabel(\"MSE\")\n",
    "    plt.ylabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(filepath_location_default+f\"__gradient_comparison__lr={lr}_lam={lam}_method={method_name}.png\")  \n",
    "    plt.show()\n",
    "\n",
    "gradient_comparison_experiment(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that anal and auto give same results, but automatic takes 10* as much time ---> we use analytic in the following!\n",
    "\n",
    "Note also that Ridge takes twice as long for auto. This might be because we have 2 terms of jnpsum(jnppower)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_momentum_experiment(method, epochs=1000, savefig=False):\n",
    "\n",
    "    # Get method name\n",
    "    method_name = next(item[\"name\"] for item in methods_dict_list if item[\"method\"] == method)\n",
    "\n",
    "    # Ensure results don't change on rerun\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Values for experiment\n",
    "    lr_vals = np.logspace(-2, 1, 6)\n",
    "    gamma_vals = np.array([0.01, 0.05, 0.2, 0.5, 0.9, 1])\n",
    "    results = np.zeros((len(lr_vals), len(gamma_vals)))\n",
    "\n",
    "    # We juse analytic OLS\n",
    "    grad_method= OLS_train_analgrad(model)\n",
    "\n",
    "    # Perform algorithm for each value\n",
    "    for i, lr in enumerate(lr_vals):\n",
    "        for j, gamma in enumerate(gamma_vals):\n",
    "\n",
    "            # Perform experiment\n",
    "            result_GD = method(X_train, y_train, X_test, y_test, \n",
    "                        grad_method=grad_method, \n",
    "                        n_epochs=epochs, \n",
    "                        lr=lr,\n",
    "                        beta0=beta0, \n",
    "                        gamma=gamma,\n",
    "                        test_loss_func=loss_func)\n",
    "\n",
    "            # Append the final error from training\n",
    "            results[i, j] = result_GD[\"test_loss_list\"][-1]\n",
    "\n",
    "    # Make plot\n",
    "    fig, ax = plt.subplots(figsize = (7, 7))\n",
    "    sns.heatmap(results, annot=True, ax=ax, cmap=\"viridis\", cbar=False)\n",
    "    ax.set_title(f\"Final test MSE loss, {method_name}\")\n",
    "\n",
    "    # Handle labels\n",
    "    ax.set_xlabel(\"Momentum parameter $\\gamma$\")\n",
    "    ax.set_ylabel(\"Logarithm of learning rate, $\\log{\\eta}$\")\n",
    "    \n",
    "    ax.set_xticklabels(np.round(gamma_vals, 2))\n",
    "    ax.set_yticklabels(np.round(np.log10(lr_vals), 2))\n",
    "\n",
    "    # Save and show    \n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(filepath_location_default+f\"__lr&momentum_eperiment__epochs={epochs}__method={method_name}.png\")  \n",
    "    plt.show()\n",
    "\n",
    "lr_momentum_experiment(GD, savefig=True)\n",
    "# lr_momentum_experiment(SGD, savefig=True)\n",
    "# lr_momentum_experiment(SGD_adagrad, savefig=True)\n",
    "# lr_momentum_experiment(SGD_RMS_prop, savefig=True)\n",
    "# lr_momentum_experiment(SGD_adam, savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here: discuss how gamma=1 results in \"shift\". Interpretation: here stack up all previous gradients. For gamma<1 we dampen the first... For gamma>1, would get exponential..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the methods for different number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We compare final train error to see \"how far convergence has come\"\n",
    "def epoch_experiment(lr, n_epochs=1000, methods_dict_list=methods_dict_list, save=False):\n",
    "\n",
    "    # Use OLS analytic\n",
    "    loss_grad = OLS_train_analgrad(model)\n",
    "\n",
    "    # For methods with n_batches, the default 5 is used!\n",
    "    n_batches = 5\n",
    "\n",
    "    for method_dict in methods_dict_list:\n",
    "\n",
    "        method_func = method_dict[\"method\"]\n",
    "        method_name = method_dict[\"name\"]\n",
    "\n",
    "        result = method_func(X_train, y_train, X_test, y_test, \n",
    "                grad_method=loss_grad, \n",
    "                n_epochs=n_epochs, \n",
    "                lr=lr,\n",
    "                beta0=beta0, \n",
    "                test_loss_func=(MSELoss_method(model)))\n",
    "            \n",
    "        if method_name==\"GD\": # batch size 1\n",
    "            plt.plot(result[\"train_loss_list\"][::1], label=method_name)\n",
    "        else:\n",
    "            plt.plot(result[\"train_loss_list\"][::n_batches], label=method_name)\n",
    "        print(f\"Final error for {method_name} = {result['train_loss_list'][-1]}\")\n",
    "\n",
    "    plt.xlabel(\"Training MSE\")\n",
    "    plt.ylabel(\"Epoch\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(filepath_location_default+f\"__epoch_eperiment__lr={lr}.png\")    \n",
    "    plt.show()\n",
    "\n",
    "# epoch_experiment(0.001)\n",
    "epoch_experiment(0.01, save=True)\n",
    "# epoch_experiment(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compare final train error to see \"how far convergence has come\"\n",
    "# Define experiment. This one alters the current plot\n",
    "def lr_experiment(lr_values, n_epochs=3, methods_dict_list=methods_dict_list, linestyle='solid'):\n",
    "\n",
    "    # For methods with n_batches, the default 5 is used!\n",
    "    # n_batches = 5\n",
    "\n",
    "    # Use OLS analytic\n",
    "    loss_grad = OLS_train_analgrad(model)\n",
    "\n",
    "    # Perform experiment for each method\n",
    "    for method_dict in methods_dict_list:\n",
    "\n",
    "        # Get name and method\n",
    "        method_func = method_dict[\"method\"]\n",
    "        method_name = method_dict[\"name\"]\n",
    "\n",
    "        # Storage for errors\n",
    "        final_errors = []\n",
    "\n",
    "        # Do experiment for each learning rate\n",
    "        for lr in lr_values:\n",
    "\n",
    "            result = method_func(X_train, y_train, X_test, y_test, \n",
    "                    grad_method=loss_grad, \n",
    "                    n_epochs=n_epochs, \n",
    "                    lr=lr,\n",
    "                    beta0=beta0, \n",
    "                    test_loss_func=(MSELoss_method(model)))\n",
    "            \n",
    "            final_errors.append(result[\"train_loss_list\"][-1])\n",
    "\n",
    "        plt.plot(lr_values, final_errors, label=method_name, color=method_dict[\"color\"], linestyle=linestyle)\n",
    "\n",
    "# Values for the learning rates\n",
    "lr_values = np.logspace(-5, 0, 19)\n",
    "\n",
    "# Colors for legend\n",
    "colors = [meth[\"color\"] for meth in methods_dict_list]\n",
    "meth_names = [meth[\"name\"] for meth in methods_dict_list]\n",
    "\n",
    "# Linestyles for legend\n",
    "epoch_nums = [3, 10, 20, 100]\n",
    "line_styles = [\"solid\", \"dotted\", \"dashed\", \"dashdot\"]\n",
    "names = [f\"{n}\" for n in epoch_nums]\n",
    "\n",
    "\n",
    "# A nice seed. Do all the experiments\n",
    "np.random.seed(35)\n",
    "for i in range(len(epoch_nums)):\n",
    "    lr_experiment(lr_values, n_epochs=epoch_nums[i], linestyle=line_styles[i])\n",
    "\n",
    "# Create custom handles for the legend, create two legends, one for color and one for line style\n",
    "color_handles = [Line2D([0], [0], color=c, lw=3) for c in colors]\n",
    "style_handles = [Line2D([0], [0], color='black', lw=3, linestyle=ls) for ls in line_styles]\n",
    "legend_color = plt.legend(color_handles, meth_names, loc='upper left', title='Method', bbox_to_anchor=(0,1))\n",
    "legend_style = plt.legend(style_handles, epoch_nums, loc='upper left', title='Epochs', bbox_to_anchor=(0.25,1), handlelength=5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(r\"Learning rate, $\\eta$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Final training error\")\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add the first legend manually to the plot\n",
    "plt.gca().add_artist(legend_color)\n",
    "plt.tight_layout()\n",
    "plt.savefig(filepath_location_default+f\"__lr_eperiment__.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batches\n",
    "\n",
    "We now se how the performance of the 4 stochastic descent methods depend on the number of minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compare final train error to see \"how far convergence has come\"\n",
    "# Define experiment. This one alters the current plot\n",
    "def minibatch_experiment(n_minibatch_values, lr=0.01, n_epochs=3, methods_dict_list=methods_dict_list, linestyle='solid'):\n",
    "\n",
    "    # Use OLS analytic\n",
    "    loss_grad = OLS_train_analgrad(model)\n",
    "\n",
    "    # Perform experiment for each method\n",
    "    for method_dict in methods_dict_list:\n",
    "\n",
    "        # Get name and method\n",
    "        method_func = method_dict[\"method\"]\n",
    "        method_name = method_dict[\"name\"]\n",
    "\n",
    "        # Storage for errors\n",
    "        final_errors = []\n",
    "\n",
    "        # Do experiment for each learning rate\n",
    "        for n_minibatch in n_minibatch_values:\n",
    "\n",
    "            result = method_func(X_train, y_train, X_test, y_test, \n",
    "                    grad_method=loss_grad, \n",
    "                    n_epochs=n_epochs,\n",
    "                    lr=lr,\n",
    "                    n_batches=n_minibatch,\n",
    "                    beta0=beta0, \n",
    "                    test_loss_func=(MSELoss_method(model)))\n",
    "            \n",
    "            final_errors.append(result[\"train_loss_list\"][-1])\n",
    "\n",
    "        plt.plot(n_minibatch_values, final_errors, label=method_name, color=method_dict[\"color\"], linestyle=linestyle)\n",
    "\n",
    "# Values for the learning rates\n",
    "n_minibach_values = [i for i in range(1, 1+40)]\n",
    "\n",
    "# Colors for legend\n",
    "colors = [meth[\"color\"] for meth in methods_dict_list]\n",
    "meth_names = [meth[\"name\"] for meth in methods_dict_list]\n",
    "\n",
    "# Linestyles for legend\n",
    "epoch_nums = [3, 10]\n",
    "line_styles = [\"solid\", \"dotted\", \"dashed\", \"dashdot\"]\n",
    "names = [f\"{n}\" for n in epoch_nums]\n",
    "\n",
    "# A nice seed. Do all the experiments\n",
    "np.random.seed(35)\n",
    "for i in range(len(epoch_nums)):\n",
    "    minibatch_experiment(n_minibach_values, n_epochs=epoch_nums[i], linestyle=line_styles[i])\n",
    "\n",
    "# Create custom handles for the legend, create two legends, one for color and one for line style\n",
    "color_handles = [Line2D([0], [0], color=c, lw=3) for c in colors]\n",
    "style_handles = [Line2D([0], [0], color='black', lw=3, linestyle=ls) for ls in line_styles]\n",
    "legend_color = plt.legend(color_handles, meth_names, loc='upper right', title='Method', bbox_to_anchor=(0.95,0.9))\n",
    "legend_style = plt.legend(style_handles, epoch_nums, loc='upper right', title='Epochs', bbox_to_anchor=(0.7,0.9), handlelength=5)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(r\"Number of minibatches\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Final training error for varying number of minibatches\")\n",
    "\n",
    "# Add the first legend manually to the plot\n",
    "plt.gca().add_artist(legend_color)\n",
    "\n",
    "# Save\n",
    "plt.tight_layout()\n",
    "plt.savefig(filepath_location_default+f\"__minibatch_eperiment__.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diskusjon. Lurt å velge 5-10 for adam selvom litt mindre feil senere. Årsak: koster å gå inn i gradientberegning, så man må *strike a balance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn Ridge\n",
    "Ridge as function of learning rate and lambda\n",
    "\n",
    "use seabord ... to show results as function of lr and lam !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_experiment(lr_vals = np.logspace(-6, 1, 6), lam_vals = np.logspace(-10, 10, 6), savefig=False):\n",
    "    # ensure the same random numbers appear every time\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Fixed parametres\n",
    "    epochs = 100\n",
    "\n",
    "    # Values for experiment\n",
    "    lr_vals = np.logspace(-6, 1, 6)\n",
    "    lam_vals = np.logspace(-10, 10, 6)\n",
    "    results = np.zeros((len(lr_vals), len(lam_vals)))\n",
    "\n",
    "    # We juse analytic ridge\n",
    "    train_grad = ridge_train_analgrad\n",
    "\n",
    "    # Perform algorithm for each value\n",
    "    for i, lr in enumerate(lr_vals):\n",
    "        for j, lam in enumerate(lam_vals):\n",
    "\n",
    "            # Get gradient function for given lambda\n",
    "            grad_method = train_grad(model, lam)\n",
    "\n",
    "            # Perform experiment\n",
    "            result_GD = SGD_adam(X_train, y_train, X_test, y_test, \n",
    "                        grad_method=grad_method, \n",
    "                        n_epochs=epochs, \n",
    "                        n_batches=5,\n",
    "                        lr=lr,\n",
    "                        beta0=beta0, \n",
    "                        test_loss_func=loss_func)\n",
    "\n",
    "            # Append the final error from training\n",
    "            results[i, j] = result_GD[\"test_loss_list\"][-1]\n",
    "\n",
    "    # Make sns heatmap\n",
    "    fig, ax = plt.subplots(figsize = (7, 7))\n",
    "    sns.heatmap(results, annot=True, ax=ax, cmap=\"viridis\", cbar=False)\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(\"Final test MSE loss\")\n",
    "\n",
    "    # Handle labels\n",
    "    ax.set_xlabel(\"Logarithm of regularisation, $\\log\\lambda$\")\n",
    "    ax.set_ylabel(\"Logarithm of learning rate, $\\log{\\eta}$\")\n",
    "\n",
    "    ax.set_xticklabels(np.round(np.log10(lam_vals), 2))\n",
    "    ax.set_yticklabels(np.round(np.log10(lr_vals), 2))\n",
    "\n",
    "    # Save/show\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(filepath_location_default+f\"__ridge_eperiment__epochs={epochs}.png\")\n",
    "    plt.show()\n",
    "\n",
    "ridge_experiment(savefig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
