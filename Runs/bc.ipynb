{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for figures for b) and c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling paths for importing code\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import split and MSELoss. All descent methods.\n",
    "from Code.utilities import train_test_split, MSELoss_method, plot_test_results\n",
    "from Code.descent_methods import *\n",
    "from Code.neural_network import _beta_init, get_neural_network_model\n",
    "\n",
    "# Import tools from the first project. Feature matrix, sampling and scaling\n",
    "from Code.project1_tools import feature_matrix_2d, r2_sampling, scale_feature_matrix, FrankeFunction\n",
    "\n",
    "# lines used for nice legend, ticker for x and y axis ticks, seaborn for grid search, pandas for nice datastorage. Numpy. jax jit to speed up\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jax import jit, grad, nn\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "\n",
    "# Timing methods\n",
    "import time as time\n",
    "\n",
    "# Import the gradient methods. Assign colors for plots comparing them ...\n",
    "from Code.utilities import ridge_loss_method\n",
    "\n",
    "methods_dict_list = [{\"name\"   : \"GD\",       \"method\" : GD,           \"color\" : \"C0\"},\n",
    "                     {\"name\"   : \"SGD\",      \"method\" : SGD,          \"color\" : \"C1\"},\n",
    "                     {\"name\"   : \"adagrad\",  \"method\" : SGD_adagrad,  \"color\" : \"C2\"},\n",
    "                     {\"name\"   : \"RMS prop\", \"method\" : SGD_RMS_prop, \"color\" : \"C3\"},\n",
    "                     {\"name\"   : \"adam\",     \"method\" : SGD_adam,     \"color\" : \"C4\"}]\n",
    "\n",
    "# Set the default font size for text elements (e.g., titles, labels)\n",
    "sns.set() # default from sns is pretties (:\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 16\n",
    "mpl.rcParams[\"axes.labelsize\"] = 14\n",
    "mpl.rcParams[\"axes.titlesize\"] = 14\n",
    "mpl.rcParams['legend.fontsize'] = 14\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# Set filename start. Create the folder if gone\n",
    "filepath_location = \"Figures/bc/\"\n",
    "if not os.path.exists(filepath_location):\n",
    "    os.makedirs(filepath_location[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix a seed for sampling and initialisation to get consistent results\n",
    "np.random.seed(41)\n",
    "\n",
    "# Sample points and split in train and test\n",
    "num_points = 100\n",
    "split=0.2\n",
    "data = r2_sampling(num_points)\n",
    "x, y, z = data[\"x\"], data[\"y\"], data[\"z\"]\n",
    "\n",
    "# Features are the scaled coordinates x and y\n",
    "X = np.array([np.array([x[i][0], y[i][0]]) for i in range(len(x))])\n",
    "X, means, var = scale_feature_matrix(X)\n",
    "\n",
    "# Activation functions\n",
    "activation_funcs = {\"ReLu\":nn.relu, \"Leaky ReLu\":nn.leaky_relu, \"Sigmoid\":nn.sigmoid, \"Tanh\":nn.tanh}\n",
    "activation_func_cols = {\"ReLu\":\"C0\", \"Leaky ReLu\":\"C1\", \"Sigmoid\":\"C2\", \"Tanh\":\"C3\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run(X, z, lr, lam, hidden_layer_num, node_per_hidden, hidden_activation, epochs=300, test_index=None, plot_or_not=False):\n",
    "    layer_list = [X.shape[1]] + [node_per_hidden]*hidden_layer_num + [1]\n",
    "    beta0 = _beta_init(layer_list)\n",
    "\n",
    "    # Create model\n",
    "    model = jit(get_neural_network_model(hidden_layer_num ,activation=hidden_activation))\n",
    "\n",
    "    # Create gradient from loss function. Ridge loss lets us use MSE by lam=0\n",
    "    loss_func = jit(ridge_loss_method(model=model, lam=lam))\n",
    "    loss_grad = jit(grad(loss_func))\n",
    "\n",
    "    # MSE loss function for training\n",
    "    test_func = jit(MSELoss_method(model))\n",
    "\n",
    "    # Split the dataset.\n",
    "    X_train, y_train, X_test, y_test, test_index = train_test_split(X, z, 0.2, test_index=test_index)\n",
    "\n",
    "    # Perform training. We use adam, add the test index\n",
    "    result = SGD_adam(X_train, y_train, X_test, y_test, grad_method=loss_grad,beta0=beta0, n_epochs=epochs, test_loss_func=test_func, lr=lr)\n",
    "    result.update({\"test_index\":test_index, \"model\":model, \"X_test\":X_test, \"y_test\":y_test})\n",
    "\n",
    "    if plot_or_not:\n",
    "        plot_test_results(result[\"test_loss_list\"], result[\"train_loss_list\"], 5, ylabel=\"MSE\")\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "result = single_run(X, z, lr=1e-3, lam=0.00, hidden_layer_num=2, node_per_hidden=10, hidden_activation=nn.sigmoid, plot_or_not=True)\n",
    "print(\"Log run\")\n",
    "#TODO LEGG TIL PÃ… SLUTTEN ETTER GOOD PARAMS HAR BLITT FUNNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_lam(savefig=False):\n",
    "    lr = 0.01\n",
    "    hidden_layer_num = 3\n",
    "    node_per_hidden = 5\n",
    "    \n",
    "    # Initialise \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    filename = filepath_location + f\"lr={lr}_Hidden_layers={hidden_layer_num}_Nodes_per_hidden={node_per_hidden}_lambda_experiment.png\"\n",
    "\n",
    "    lams = np.logspace(-10, 4, num=30)\n",
    "    test_errs_all = {}\n",
    "    train_errs_all = {}\n",
    "    result = {}\n",
    "    for act_func in activation_funcs.keys():\n",
    "        test_errs = []\n",
    "        train_errs = []\n",
    "        for lam in tqdm(lams):\n",
    "            #tqdm.write(f\"Lambda: {lam}\")\n",
    "            result = single_run(X, z, lr=lr, lam=lam, hidden_layer_num=hidden_layer_num, node_per_hidden=node_per_hidden, hidden_activation=activation_funcs[act_func], test_index=result.get(\"test_index\"))\n",
    "            test_errs.append(result[\"test_loss_list\"][-1])\n",
    "            train_errs.append(result[\"train_loss_list\"][-1])\n",
    "        test_errs_all[act_func] = test_errs\n",
    "        train_errs_all[act_func] = train_errs\n",
    "\n",
    "        plt.plot(lams, test_errs, label=act_func, color=activation_func_cols[act_func])\n",
    "        plt.plot(lams, train_errs, color=activation_func_cols[act_func], linestyle=\"dotted\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Final error for varying regularization parameters\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xlabel(r\"$\\lambda$\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    return {\"lams\":lams, \"test_error\":test_errs_all, \"train_error\":train_errs_all}\n",
    "\n",
    "experiment = search_lam(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_lrs(savefig=False, nreps=5):\n",
    "    lam = 1e-5\n",
    "    hidden_layer_num = 3\n",
    "    node_per_hidden = 5\n",
    "    \n",
    "    # Initialise \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    filename = filepath_location + f\"lambda={lam}_Hidden_layers={hidden_layer_num}_Nodes_per_hidden={node_per_hidden}_nreps={nreps}_lr_experiment.png\"\n",
    "\n",
    "    lrs = np.logspace(-8, 0, num=30)\n",
    "\n",
    "    test_errs_all = {}\n",
    "    train_errs_all = {}\n",
    "\n",
    "    result = {}\n",
    "    for act_func in activation_funcs.keys():\n",
    "        test_errs = []\n",
    "        train_errs = []\n",
    "        for lr in tqdm(lrs):\n",
    "            #tqdm.write(f\"Learning rate: {lr}\")\n",
    "            s_test = 0.0\n",
    "            s_train = 0.0\n",
    "            for i in range(nreps):\n",
    "                result = single_run(X, z, lr=lr, lam=lam, hidden_layer_num=hidden_layer_num, node_per_hidden=node_per_hidden, hidden_activation=activation_funcs[act_func], test_index=result.get(\"test_index\"))\n",
    "                s_test += result[\"test_loss_list\"][-1]\n",
    "                s_train += result[\"train_loss_list\"][-1]\n",
    "            test_errs.append(s_test/nreps)\n",
    "            train_errs.append(s_train/nreps)\n",
    "\n",
    "        test_errs_all[act_func] = test_errs\n",
    "        train_errs_all[act_func] = train_errs\n",
    "\n",
    "        plt.plot(lrs, test_errs, label=act_func, color=activation_func_cols[act_func])\n",
    "        plt.plot(lrs, train_errs, color=activation_func_cols[act_func], linestyle=\"dotted\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Final error for varying learning rates\")\n",
    "    plt.ylabel(\"Average MSE\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel(r\"Learning Rate, $\\eta$\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    return {\"lrs\":lrs, \"test_error\":test_errs_all, \"train_error\":train_errs_all}\n",
    "\n",
    "\n",
    "#learning_rate_exp = search_lrs()\n",
    "l = search_lrs(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hidden_layer_sizes(savefig=False, nreps=5):\n",
    "    lr = 0.01\n",
    "    lam = 1e-5\n",
    "    hidden_layer_num = 1\n",
    "\n",
    "    # Init\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    filename = filepath_location + f\"lr={lr}_lambda={lam}_Hidden_layers={hidden_layer_num}_nreps={nreps}_hidden_layer_size_experiment.png\"\n",
    "\n",
    "    hidden_layer_sizes = np.arange(1, 30)\n",
    "\n",
    "    test_errs_all = {}\n",
    "    train_errs_all = {}\n",
    "    result = {}\n",
    "    for act_func in activation_funcs.keys():\n",
    "        test_errs = []\n",
    "        train_errs = []\n",
    "        for hidden_layer_size in tqdm(hidden_layer_sizes):\n",
    "            #tqdm.write(f\"Hidden Layer size: {hidden_layer_size}\")\n",
    "            s_test = 0.0\n",
    "            s_train = 0.0\n",
    "            for i in range(nreps):\n",
    "                result = single_run(X, z, lr=lr, lam=lam, hidden_layer_num=hidden_layer_num, node_per_hidden=hidden_layer_size, hidden_activation=activation_funcs[act_func], test_index=result.get(\"test_index\"))\n",
    "                s_test += result[\"test_loss_list\"][-1]\n",
    "                s_train += result[\"train_loss_list\"][-1]\n",
    "            test_errs.append(s_test/nreps)\n",
    "            train_errs.append(s_train/nreps)\n",
    "\n",
    "        test_errs_all[act_func] = test_errs\n",
    "        train_errs_all[act_func] = train_errs\n",
    "\n",
    "        plt.plot(hidden_layer_sizes, test_errs, label=act_func, color=activation_func_cols[act_func])\n",
    "        plt.plot(hidden_layer_sizes, train_errs, color=activation_func_cols[act_func], linestyle=\"dotted\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Final error for varying number of nodes\")\n",
    "    plt.ylabel(\"Average MSE\")\n",
    "    plt.xlabel(\"Nodes per layer\")\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    return {\"hidden_layer_sizes\":hidden_layer_sizes, \"test_error\":test_errs_all, \"train_error\":train_errs_all}\n",
    "\n",
    "\n",
    "#learning_rate_exp = search_lrs()\n",
    "l = search_hidden_layer_sizes(savefig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hidden_layer_number(savefig=False, nreps=10):\n",
    "    lr = 0.01\n",
    "    lam = 1e-5\n",
    "    node_per_hidden = 5\n",
    "    \n",
    "    \n",
    "    filename = filepath_location + f\"lr={lr}_lambda={lam}_Nodes_per_hidden={node_per_hidden}_nreps={nreps}_hidden_layer_number_experiment.png\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    hidden_layer_numbers = np.arange(1, 9)\n",
    "\n",
    "    test_errs_all = {}\n",
    "    train_errs_all = {}\n",
    "    result = {}\n",
    "    for act_func in activation_funcs.keys():\n",
    "        test_errs = []\n",
    "        train_errs = []\n",
    "        for hidden_layer_number in tqdm(hidden_layer_numbers):\n",
    "            #tqdm.write(f\"Hidden Layer number: {hidden_layer_number}\")\n",
    "            s_test = 0.0\n",
    "            s_train = 0.0\n",
    "            for i in range(nreps):\n",
    "                result = single_run(X, z, lr=lr, lam=lam, hidden_layer_num=hidden_layer_number, node_per_hidden=node_per_hidden, hidden_activation=activation_funcs[act_func], test_index=result.get(\"test_index\"))\n",
    "                s_test += result[\"test_loss_list\"][-1]\n",
    "                s_train += result[\"train_loss_list\"][-1]\n",
    "\n",
    "            test_errs.append(s_test/nreps)\n",
    "            train_errs.append(s_train/nreps)\n",
    "\n",
    "        test_errs_all[act_func] = test_errs\n",
    "        train_errs_all[act_func] = train_errs\n",
    "        \n",
    "        plt.plot(hidden_layer_numbers, test_errs, label=act_func, color=activation_func_cols[act_func])\n",
    "        plt.plot(hidden_layer_numbers, train_errs, color=activation_func_cols[act_func], linestyle=\"dotted\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Final error for varying number of hidden layers\")\n",
    "    plt.ylabel(\"Average MSE\")\n",
    "    plt.xlabel(\"Number of hidden layers\")\n",
    "    if savefig:\n",
    "        plt.savefig(filename)  \n",
    "    plt.show()\n",
    "\n",
    "    return {\"hidden_layer_sizes\":hidden_layer_numbers, \"test_error\":test_errs_all, \"train_error\":train_errs_all}\n",
    "\n",
    "\n",
    "#learning_rate_exp = search_lrs()\n",
    "l = search_hidden_layer_number(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"axes.labelsize\"] = 17\n",
    "mpl.rcParams[\"axes.titlesize\"] = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_hidden_layer_grid(savefig=False, nreps=5):\n",
    "    lam = 1e-5\n",
    "    node_per_hidden = 5\n",
    "    hidden_activation = nn.tanh\n",
    "    hidden_activation_name = \"tanh\"\n",
    "    \n",
    "    n = 6\n",
    "    \n",
    "    filename = filepath_location + f\"lambda={lam}_Nodes_per_hidden={node_per_hidden}_hidden_activation_func={hidden_activation_name}_nreps={nreps}_lr_hidden_layer_num_grid_experiment.png\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "    hidden_layer_numbers = np.arange(1, n + 1)\n",
    "    lrs = np.logspace(-6, 0, n)\n",
    "\n",
    "    test_errs_all = np.zeros(shape=(n, n))\n",
    "    train_errs_all = np.zeros(shape=(n, n))\n",
    "    result = {}\n",
    "    for i, lr in enumerate(lrs):\n",
    "        print(f\"lr: {lr}\")\n",
    "        for j, hidden_layer_number in enumerate(hidden_layer_numbers):\n",
    "            #tqdm.write(f\"Learning rate: {lr}, Hidden Layer number: {hidden_layer_number}\")\n",
    "            s_test = 0.0\n",
    "            s_train = 0.0\n",
    "            for k in range(nreps):\n",
    "                result = single_run(X, z, lr=lr, lam=lam, hidden_layer_num=hidden_layer_number, node_per_hidden=node_per_hidden, hidden_activation=hidden_activation, test_index=result.get(\"test_index\"))\n",
    "                s_test += result[\"test_loss_list\"][-1]\n",
    "                s_train += result[\"train_loss_list\"][-1]\n",
    "\n",
    "            test_errs_all[i, j] = s_test/nreps\n",
    "            train_errs_all[i, j] = s_train/nreps\n",
    "\n",
    "\n",
    "    # Make sns heatmap\n",
    "    sns.heatmap(test_errs_all, annot=True, ax=ax, cmap=\"viridis_r\", cbar=True, norm=\"log\")\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(\"Final test MSE\")\n",
    "\n",
    "    # Handle labels\n",
    "    ax.set_xlabel(\"Number of hidden layers\")\n",
    "    ax.set_ylabel(\"Logarithm of learning rate, $\\log{\\eta}$\")\n",
    "\n",
    "    ax.set_yticklabels(np.round(np.log10(lrs), 2))\n",
    "    ax.set_xticklabels((hidden_layer_numbers))\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return {\"hidden_layer_sizes\":hidden_layer_numbers, \"test_error\":test_errs_all, \"train_error\":train_errs_all}\n",
    "\n",
    "\n",
    "r= learning_rate_hidden_layer_grid(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal arhitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_node_hidden_layer_grid(savefig=False, nreps=5):\n",
    "    lam = 1e-5\n",
    "    lr = 0.01\n",
    "    hidden_activation = nn.tanh\n",
    "    hidden_activation_name = \"tanh\"\n",
    "    \n",
    "    n = 6\n",
    "    \n",
    "    filename = filepath_location + f\"lr={lr}_lambda={lam}_hidden_activation_func={hidden_activation_name}_nreps={nreps}__hidden_layer_num_grid_experiment.png\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "\n",
    "    hidden_layer_numbers = np.arange(1, n + 1)\n",
    "    nodes_per_layer_list = np.arange(1, 19, 19//n)\n",
    "\n",
    "    test_errs_all = np.zeros(shape=(n, n))\n",
    "    train_errs_all = np.zeros(shape=(n, n))\n",
    "    result = {}\n",
    "    for i, nodes_per_layer in enumerate(nodes_per_layer_list):\n",
    "        print(f\"nodes per layer: {nodes_per_layer}\")\n",
    "        for j, hidden_layer_number in enumerate(hidden_layer_numbers):\n",
    "            #tqdm.write(f\"Learning rate: {lr}, Hidden Layer number: {hidden_layer_number}\")\n",
    "            s_test = 0.0\n",
    "            s_train = 0.0\n",
    "            for k in range(nreps):\n",
    "                result = single_run(X, z, lr=lr, lam=lam, hidden_layer_num=hidden_layer_number, node_per_hidden=nodes_per_layer, hidden_activation=hidden_activation, test_index=result.get(\"test_index\"))\n",
    "                s_test += result[\"test_loss_list\"][-1]\n",
    "                s_train += result[\"train_loss_list\"][-1]\n",
    "\n",
    "            test_errs_all[i, j] = s_test/nreps\n",
    "            train_errs_all[i, j] = s_train/nreps\n",
    "\n",
    "\n",
    "    sns.heatmap(test_errs_all, annot=True, ax=ax, cmap=\"viridis_r\", cbar=True, norm=\"log\")\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(\"Final test MSE\")\n",
    "\n",
    "    # Handle labels\n",
    "    ax.set_xlabel(\"Number of hidden layers\")\n",
    "    ax.set_ylabel(\"Number of nodes per layer\")\n",
    "\n",
    "    ax.set_yticklabels(nodes_per_layer_list)\n",
    "    ax.set_xticklabels(hidden_layer_numbers)\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        fig.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    return {\"hidden_layer_sizes\":hidden_layer_numbers, \"test_error\":test_errs_all, \"train_error\":train_errs_all}\n",
    "\n",
    "\n",
    "r= hidden_node_hidden_layer_grid(savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the result of the best prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "mpl.rcParams['font.size'] = 56\n",
    "mpl.rcParams[\"axes.labelsize\"] = 54\n",
    "mpl.rcParams[\"axes.titlesize\"] = 30\n",
    "mpl.rcParams['legend.fontsize'] = 25\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "def plot_surface(xp, yp, zp, test_index,x_mesh,y_mesh,z_mesh,filename=None):\n",
    "\n",
    "    # Init figure\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    # ax1.tit\n",
    "    ax2 = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "\n",
    "    ax3 = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "\n",
    "    # Plot the surface.\n",
    "    surf1 = ax1.contourf(x_mesh, y_mesh, FrankeFunction(x_mesh, y_mesh), cmap=cm.coolwarm, levels=100)\n",
    "    ax1.contour(x_mesh, y_mesh, FrankeFunction(x_mesh, y_mesh), cmap=cm.coolwarm)\n",
    "    surf2 = ax2.plot_surface(x_mesh, y_mesh, FrankeFunction(x_mesh, y_mesh), cmap=cm.coolwarm,\n",
    "                            linewidth=0, antialiased=False, alpha=1)\n",
    "    surf3 = ax3.plot_surface(x_mesh, y_mesh, z_mesh, cmap=cm.coolwarm,\n",
    "                            linewidth=0, antialiased=False)\n",
    "\n",
    "    # Add the test and train points to the original function\n",
    "    ax1.scatter(xp[test_index], yp[test_index],s=20, color=\"red\", zorder=1, label=\"Test\")\n",
    "    ax1.scatter(xp[np.logical_not(np.isin(np.arange(len(xp)), test_index))], \n",
    "                  yp[np.logical_not(np.isin(np.arange(len(yp)), test_index))], \n",
    "                  s=20, color=\"green\", zorder=1, label=\"Train\")\n",
    "    # ax2.scatter(xp[test_index], yp[test_index], zp[test_index], color=\"red\", zorder=4, label=\"Test\")\n",
    "    # ax2.scatter(xp[np.logical_not(np.isin(np.arange(len(xp)), test_index))], \n",
    "    #               yp[np.logical_not(np.isin(np.arange(len(yp)), test_index))],\n",
    "    #               zp[np.logical_not(np.isin(np.arange(len(zp)), test_index))], \n",
    "    #               color=\"green\", zorder=4, label=\"Train\")\n",
    "  \n",
    "    ax1.set_title(\"Franke Function with data points\")\n",
    "    # ax1.legend()\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    ax2.view_init(elev=45, azim=30)\n",
    "    ax2.set_title(\"Franke Function surface\")\n",
    "\n",
    "    ax3.view_init(elev=45, azim=30)\n",
    "    ax3.set_title(\"Model surface\")\n",
    "\n",
    "    fig.subplots_adjust(wspace=0)\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_neural_network_franke_prediction(xp, yp, zp, test_index, beta_dict, means, var, filename=None):\n",
    "\n",
    "    # Number of points to use\n",
    "    num_plot = 101\n",
    "\n",
    "    # Make data.\n",
    "    x = np.linspace(0, 1, num_plot).reshape((num_plot, 1))\n",
    "    y = np.linspace(0, 1, num_plot).reshape((num_plot, 1))\n",
    "\n",
    "    x_mesh = np.zeros(shape=(num_plot*num_plot, 1))\n",
    "    y_mesh = np.zeros(shape=(num_plot*num_plot, 1))\n",
    "\n",
    "    for i in range(num_plot):\n",
    "        for j in range(num_plot):\n",
    "            x_mesh[j*num_plot + i] = x[i, 0]\n",
    "            y_mesh[j*num_plot + i] = y[j, 0]\n",
    "    \n",
    "    # Feature matrix of points to plot\n",
    "    X = (np.array([np.array([x[i][0], y[j][0]]) for i in range(len(x)) for j in range(len(y))]) - means) / np.sqrt(var)\n",
    "    \n",
    "    # Make prediction\n",
    "    z = np.array([s[0] for s in model(beta_dict, X)])\n",
    "    # Reshaoe\n",
    "    x_mesh = x_mesh.reshape(num_plot, num_plot)\n",
    "    y_mesh = y_mesh.reshape(num_plot, num_plot)\n",
    "    z_mesh = z.reshape(num_plot, num_plot)\n",
    "\n",
    "    # Plot surface\n",
    "    plot_surface(xp, yp, zp,test_index, x_mesh,y_mesh,z_mesh,filename=filename)\n",
    "\n",
    "### Fix a seed for sampling and initialisation to get consistent results\n",
    "np.random.seed(41)\n",
    "\n",
    "# Sample points and split in train and test\n",
    "num_points = 100\n",
    "split=0.2\n",
    "data = r2_sampling(num_points)\n",
    "x, y, z = data[\"x\"], data[\"y\"], data[\"z\"]\n",
    "\n",
    "# Features are the scaled coordinates x and y\n",
    "X = np.array([np.array([y[i][0], x[i][0]]) for i in range(len(x))])\n",
    "X, means, var = scale_feature_matrix(X)\n",
    "X_train, y_train, X_test, y_test, test_index = train_test_split(X, z, split)\n",
    "\n",
    "N_hidden = 8\n",
    "node_hidden = 20\n",
    "activation = nn.tanh\n",
    "\n",
    "model = get_neural_network_model(N_hidden, activation=activation)\n",
    "result = single_run(X, z, lr=0.01, lam=0.00001, hidden_layer_num=N_hidden, node_per_hidden=node_hidden, hidden_activation=activation, epochs=1000, plot_or_not=False, test_index=test_index)\n",
    "plot_neural_network_franke_prediction(x, y, z, test_index, result[\"beta_final\"], means, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"test_loss_list\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for the surfaces\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z1 = np.sin(np.sqrt(x**2 + y**2))\n",
    "z2 = np.cos(np.sqrt(x**2 + y**2))\n",
    "\n",
    "# Create some random data for the scatter plot\n",
    "num_points = 100\n",
    "scatter_x = np.random.uniform(-5, 5, num_points)\n",
    "scatter_y = np.random.uniform(-5, 5, num_points)\n",
    "scatter_z = np.sin(np.sqrt(scatter_x**2 + scatter_y**2))\n",
    "\n",
    "# Create a 1x2 subplot grid\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot the first surface in the first subplot\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.scatter(scatter_x, scatter_y, scatter_z, c='red', marker='o', label='Scatter Points', zorder=1)\n",
    "ax1.plot_surface(x, y, z1, cmap='viridis', alpha=0.8)  # Adding alpha for transparency\n",
    "ax1.set_title('Scatter on Top of Surface')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the second surface in the second subplot\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.plot_surface(x, y, z2, cmap='plasma')\n",
    "ax2.set_title('Surface 2')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plt.axes(projection='3d', elev=35, azim=-125)\n",
    "ax.plot_surface(x, y, z, cmap=plt.cm.coolwarm, linewidth=0.1, zorder=1)\n",
    "ax.plot(scatter_x, scatter_y, scatter_z, 'b.', markersize=10, label='top', zorder=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
